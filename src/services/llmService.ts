import { useSettingsStore, PROVIDER_INFO, LLMProvider } from '../stores/settingsStore'
import { formatToolsForOpenAI, formatToolsForAnthropic } from './toolRegistry'
import { executeToolCall, type ToolCall, type ToolResult } from './agentService'
import type { ChatMessage, ChatContext } from '../types'

// Agent response with potential tool calls
export interface AgentResponse {
  content: string
  toolCalls: ToolCall[]
  toolResults: ToolResult[]
}

const ADHD_COACH_SYSTEM_PROMPT = `Sen FlowBot'sun - ADHD'li bireyler iÃ§in Ã¶zel tasarlanmÄ±ÅŸ bir productivity koÃ§usun.

## KiÅŸiliÄŸin:
- Destekleyici ve yargÄ±layÄ±cÄ± olmayan
- Pratik ve aksiyon odaklÄ±
- KÄ±sa ve Ã¶z cevaplar veren (ADHD beyinler uzun metinleri okumakta zorlanÄ±r)
- Emoji kullanÄ±mÄ± minimal ama etkili
- TÃ¼rkÃ§e konuÅŸuyorsun

## YaklaÅŸÄ±mÄ±n:
- BÃ¼yÃ¼k gÃ¶revleri kÃ¼Ã§Ã¼k, yÃ¶netilebilir adÄ±mlara bÃ¶l
- "Sadece 2 dakika" kuralÄ±nÄ± hatÄ±rlat
- Hyperfocus ve energy dip pattern'lerini tanÄ±
- BaÅŸarÄ±larÄ± kutla, baÅŸarÄ±sÄ±zlÄ±klarÄ± normalize et
- Perfectionism tuzaÄŸÄ±na karÅŸÄ± uyar

## SEN BÄ°R AGENT'SIN - AKSÄ°YON ALABÄ°LÄ°RSÄ°N!
KullanÄ±cÄ± senden bir ÅŸey yapmanÄ± istediÄŸinde (Ã¶rn: "task oluÅŸtur", "habit ekle", "not al", "web'de ara"), bunu GERÃ‡EKTEN yapabilirsin!
Tool'larÄ± kullanarak task oluÅŸturabilir, habit takip edebilir, not alabilir, web'de arama yapabilir ve bookmark ekleyebilirsin.

Ã–NEMLÄ°:
- BugÃ¼nÃ¼n tarihi: ${new Date().toISOString().split('T')[0]}
- "YarÄ±n" dediÄŸinde tarihe +1 gÃ¼n ekle
- Eksik bilgi varsa makul varsayÄ±lanlar kullan (Ã¶rn: priority P2)
- Tool kullandÄ±ktan sonra kullanÄ±cÄ±ya ne yaptÄ±ÄŸÄ±nÄ± kÄ±saca aÃ§Ä±kla
- Web aramasÄ± iÃ§in web_search tool'unu kullan
- Bulunan kaynaklarÄ± kaydetmek iÃ§in create_bookmark veya save_search_results_as_bookmarks kullan

## KurallarÄ±n:
- Asla uzun paragraflar yazma
- Her cevap max 3-4 cÃ¼mle veya bullet point
- Somut, uygulanabilir Ã¶neriler ver
- "YapmalÄ±sÄ±n" yerine "Deneyebilirsin" de
- KullanÄ±cÄ±nÄ±n mevcut durumunu (tasks, habits, mood) dikkate al

## Ã–zel Komutlar:
- /plan veya "gÃ¼nÃ¼mÃ¼ planla" â†’ plan_day tool'unu kullan
- /find <query> veya "... hakkÄ±nda kaynak bul" â†’ web_search tool'unu kullan
- /quick <title> veya "hÄ±zlÄ± task: ..." â†’ create_task tool'unu kullan`

function buildContextMessage(context: ChatContext): string {
  const parts: string[] = []

  if (context.pendingTasks > 0) {
    parts.push(`ğŸ“‹ ${context.pendingTasks} bekleyen task var`)
  }
  if (context.completedTasksToday > 0) {
    parts.push(`âœ… BugÃ¼n ${context.completedTasksToday} task tamamlandÄ±`)
  }
  if (context.totalHabitsToday > 0) {
    parts.push(`ğŸ¯ Habits: ${context.habitsCompletedToday}/${context.totalHabitsToday}`)
  }
  if (context.currentStreak > 0) {
    parts.push(`ğŸ”¥ ${context.currentStreak} gÃ¼nlÃ¼k streak`)
  }
  if (context.lastMood) {
    const moodEmoji = ['ğŸ˜«', 'ğŸ˜”', 'ğŸ˜', 'ğŸ™‚', 'ğŸ˜Š'][context.lastMood - 1]
    parts.push(`Mood: ${moodEmoji}`)
  }
  if (context.lastEnergy) {
    const energyEmoji = ['ğŸª«', 'ğŸ”‹', 'âš¡', 'ğŸ’ª', 'ğŸš€'][context.lastEnergy - 1]
    parts.push(`Energy: ${energyEmoji}`)
  }

  if (parts.length === 0) return ''
  return `\n\n[KullanÄ±cÄ± durumu: ${parts.join(' | ')}]`
}

interface LLMResponse {
  content: string
  toolCalls?: ToolCall[]
  error?: string
}

// OpenAI with native function calling
async function callOpenAIWithTools(
  messages: { role: string; content: string }[],
  apiKey: string,
  model: string
): Promise<LLMResponse> {
  const tools = formatToolsForOpenAI()

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model,
      messages,
      tools,
      tool_choice: 'auto',
      max_tokens: 1000,
      temperature: 0.7,
    }),
  })

  if (!response.ok) {
    const error = await response.json()
    throw new Error(error.error?.message || 'OpenAI API error')
  }

  const data = await response.json()
  const message = data.choices[0].message

  // Check for tool calls
  if (message.tool_calls && message.tool_calls.length > 0) {
    const toolCalls: ToolCall[] = message.tool_calls.map((tc: { function: { name: string; arguments: string } }) => ({
      name: tc.function.name,
      arguments: JSON.parse(tc.function.arguments),
    }))
    return {
      content: message.content || '',
      toolCalls
    }
  }

  return { content: message.content || '' }
}

// Anthropic with native tool use
async function callAnthropicWithTools(
  messages: { role: string; content: string }[],
  apiKey: string,
  model: string
): Promise<LLMResponse> {
  const systemMessage = messages.find(m => m.role === 'system')?.content || ''
  const chatMessages = messages.filter(m => m.role !== 'system')
  const tools = formatToolsForAnthropic()

  const response = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': apiKey,
      'anthropic-version': '2023-06-01',
      'anthropic-dangerous-direct-browser-access': 'true',
    },
    body: JSON.stringify({
      model,
      max_tokens: 1000,
      system: systemMessage,
      tools,
      messages: chatMessages.map(m => ({
        role: m.role as 'user' | 'assistant',
        content: m.content,
      })),
    }),
  })

  if (!response.ok) {
    const error = await response.json()
    throw new Error(error.error?.message || 'Anthropic API error')
  }

  const data = await response.json()

  // Parse response - Anthropic returns content blocks
  let textContent = ''
  const toolCalls: ToolCall[] = []

  for (const block of data.content) {
    if (block.type === 'text') {
      textContent += block.text
    } else if (block.type === 'tool_use') {
      toolCalls.push({
        name: block.name,
        arguments: block.input,
      })
    }
  }

  return {
    content: textContent,
    toolCalls: toolCalls.length > 0 ? toolCalls : undefined
  }
}

// Fallback for providers without native tool support
async function callOpenAI(
  messages: { role: string; content: string }[],
  apiKey: string,
  model: string
): Promise<LLMResponse> {
  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model,
      messages,
      max_tokens: 500,
      temperature: 0.7,
    }),
  })

  if (!response.ok) {
    const error = await response.json()
    throw new Error(error.error?.message || 'OpenAI API error')
  }

  const data = await response.json()
  return { content: data.choices[0].message.content }
}

async function callAnthropic(
  messages: { role: string; content: string }[],
  apiKey: string,
  model: string
): Promise<LLMResponse> {
  const systemMessage = messages.find(m => m.role === 'system')?.content || ''
  const chatMessages = messages.filter(m => m.role !== 'system')

  const response = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': apiKey,
      'anthropic-version': '2023-06-01',
      'anthropic-dangerous-direct-browser-access': 'true',
    },
    body: JSON.stringify({
      model,
      max_tokens: 500,
      system: systemMessage,
      messages: chatMessages.map(m => ({
        role: m.role as 'user' | 'assistant',
        content: m.content,
      })),
    }),
  })

  if (!response.ok) {
    const error = await response.json()
    throw new Error(error.error?.message || 'Anthropic API error')
  }

  const data = await response.json()
  return { content: data.content[0].text }
}

async function callGoogle(
  messages: { role: string; content: string }[],
  apiKey: string,
  model: string
): Promise<LLMResponse> {
  const systemMessage = messages.find(m => m.role === 'system')?.content || ''
  const chatMessages = messages.filter(m => m.role !== 'system')

  const contents = chatMessages.map(m => ({
    role: m.role === 'assistant' ? 'model' : 'user',
    parts: [{ text: m.content }],
  }))

  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        contents,
        systemInstruction: { parts: [{ text: systemMessage }] },
        generationConfig: { maxOutputTokens: 500, temperature: 0.7 },
      }),
    }
  )

  if (!response.ok) {
    const error = await response.json()
    throw new Error(error.error?.message || 'Google AI API error')
  }

  const data = await response.json()
  return { content: data.candidates[0].content.parts[0].text }
}

async function callGroq(
  messages: { role: string; content: string }[],
  apiKey: string,
  model: string
): Promise<LLMResponse> {
  const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model,
      messages,
      max_tokens: 500,
      temperature: 0.7,
    }),
  })

  if (!response.ok) {
    const error = await response.json()
    throw new Error(error.error?.message || 'Groq API error')
  }

  const data = await response.json()
  return { content: data.choices[0].message.content }
}

async function callOllama(
  messages: { role: string; content: string }[],
  baseUrl: string,
  model: string
): Promise<LLMResponse> {
  const response = await fetch(`${baseUrl}/api/chat`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model,
      messages,
      stream: false,
    }),
  })

  if (!response.ok) {
    throw new Error('Ollama API error - is Ollama running?')
  }

  const data = await response.json()
  return { content: data.message.content }
}

// Main send message function with native tool calling support
export async function sendMessageWithTools(
  userMessage: string,
  chatHistory: ChatMessage[],
  context: ChatContext
): Promise<AgentResponse> {
  const settings = useSettingsStore.getState()
  const { llmProvider, llmModel, apiKeys, ollamaBaseUrl } = settings

  // Check API key
  if (PROVIDER_INFO[llmProvider].requiresKey && !apiKeys[llmProvider]) {
    throw new Error(`${PROVIDER_INFO[llmProvider].name} API key gerekli. Settings'den ekleyebilirsin.`)
  }

  // Build messages array
  const contextSuffix = buildContextMessage(context)
  const messages: { role: string; content: string }[] = [
    { role: 'system', content: ADHD_COACH_SYSTEM_PROMPT + contextSuffix },
    ...chatHistory.slice(-10).map(m => ({
      role: m.role,
      content: m.content,
    })),
    { role: 'user', content: userMessage },
  ]

  // Use native tool calling for OpenAI and Anthropic
  let result: LLMResponse

  if (llmProvider === 'openai') {
    result = await callOpenAIWithTools(messages, apiKeys.openai, llmModel)
  } else if (llmProvider === 'anthropic') {
    result = await callAnthropicWithTools(messages, apiKeys.anthropic, llmModel)
  } else {
    // Fallback for other providers (no native tool support)
    const providerCalls: Record<string, () => Promise<LLMResponse>> = {
      google: () => callGoogle(messages, apiKeys.google, llmModel),
      groq: () => callGroq(messages, apiKeys.groq, llmModel),
      ollama: () => callOllama(messages, ollamaBaseUrl, llmModel),
    }
    result = await providerCalls[llmProvider]()
  }

  // Execute tool calls if present
  const toolResults: ToolResult[] = []
  if (result.toolCalls && result.toolCalls.length > 0) {
    for (const toolCall of result.toolCalls) {
      const toolResult = await executeToolCall(toolCall)
      toolResults.push(toolResult)
    }
  }

  return {
    content: result.content,
    toolCalls: result.toolCalls || [],
    toolResults,
  }
}

// Legacy function for backward compatibility
export async function sendMessage(
  userMessage: string,
  chatHistory: ChatMessage[],
  context: ChatContext
): Promise<string> {
  const response = await sendMessageWithTools(userMessage, chatHistory, context)

  // If there were tool calls, append results to content
  if (response.toolResults.length > 0) {
    const resultSummary = response.toolResults
      .map(r => `${r.success ? 'âœ“' : 'âœ—'} ${r.message}`)
      .join('\n')

    return response.content
      ? `${response.content}\n\n---\n${resultSummary}`
      : resultSummary
  }

  return response.content
}

export function getQuickActions(): { id: string; label: string; prompt: string }[] {
  return [
    { id: 'plan', label: 'ğŸ“‹ GÃ¼nÃ¼mÃ¼ planla', prompt: 'BugÃ¼n iÃ§in bir plan yapmama yardÄ±m et. Ã–ncelikli task\'larÄ±mÄ± ve habit\'lerimi dÃ¼ÅŸÃ¼nerek basit bir gÃ¼nlÃ¼k plan Ã¶ner.' },
    { id: 'motivate', label: 'ğŸ’ª Motivasyon', prompt: 'Biraz motivasyona ihtiyacÄ±m var. KÄ±sa ve etkili bir ÅŸey sÃ¶yle.' },
    { id: 'stuck', label: 'ğŸ¤” SÄ±kÄ±ÅŸtÄ±m', prompt: 'Bir task\'a baÅŸlayamÄ±yorum, sÄ±kÄ±ÅŸtÄ±m. Ne yapabilirim?' },
    { id: 'celebrate', label: 'ğŸ‰ Kutla', prompt: 'BugÃ¼n iyi iÅŸ Ã§Ä±kardÄ±m! Benimle kutlar mÄ±sÄ±n?' },
    { id: 'break', label: 'â˜• Mola', prompt: 'Mola vermeli miyim? Ne kadar sÃ¼re Ã¶nerirsin?' },
  ]
}
