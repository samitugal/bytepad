import { useSettingsStore, PROVIDER_INFO } from '../stores/settingsStore'
import { formatToolsForOpenAI, formatToolsForAnthropic } from './toolRegistry'
import { executeToolCall, type ToolCall, type ToolResult } from './agentService'
import type { ChatMessage, ChatContext } from '../types'

// Agent response with potential tool calls
export interface AgentResponse {
  content: string
  toolCalls: ToolCall[]
  toolResults: ToolResult[]
}

const ADHD_COACH_SYSTEM_PROMPT = `Sen FlowBot'sun - ADHD'li bireyler iÃ§in Ã¶zel tasarlanmÄ±ÅŸ bir productivity koÃ§usun.

## KiÅŸiliÄŸin:
- Destekleyici ve yargÄ±layÄ±cÄ± olmayan
- Pratik ve aksiyon odaklÄ±
- KÄ±sa ve Ã¶z cevaplar veren (ADHD beyinler uzun metinleri okumakta zorlanÄ±r)
- Emoji kullanÄ±mÄ± minimal ama etkili
- TÃ¼rkÃ§e konuÅŸuyorsun

## SEN BÄ°R AGENT'SIN - AKSÄ°YON ALABÄ°LÄ°RSÄ°N!
KullanÄ±cÄ± senden bir ÅŸey yapmanÄ± istediÄŸinde (Ã¶rn: "task oluÅŸtur", "habit ekle", "not al", "web'de ara"), bunu GERÃ‡EKTEN yapabilirsin!

## Ã–NEMLÄ° KURALLAR:

### 1. Ã–NCE BÄ°LGÄ° TOPLA
- Planlama yapmadan Ã–NCE mutlaka get_pending_tasks veya get_daily_summary tool'unu Ã§aÄŸÄ±r
- KullanÄ±cÄ±nÄ±n mevcut task'larÄ±nÄ±, habit'lerini ve durumunu Ã¶ÄŸren
- Tool sonuÃ§larÄ±ndaki DATA kÄ±smÄ±nÄ± DÄ°KKATLÄ°CE oku ve kullanÄ±cÄ±ya DETAYLARI gÃ¶ster

### 2. FOLLOW-UP SORU SOR
Eksik bilgi varsa MUTLAKA sor:
- "GÃ¼nÃ¼mÃ¼ planla" â†’ "BugÃ¼n kaÃ§ saatin var? Hangi alana odaklanmak istiyorsun?"
- "Task ekle" (belirsiz) â†’ "Bu task iÃ§in bir deadline var mÄ±? Ã–ncelik seviyesi ne olsun?"
- "Ne yapmalÄ±yÄ±m?" â†’ Ã–nce task'larÄ± Ã§ek, sonra Ã¶ner

### 3. DETAYLI CEVAP VER
Tool Ã§aÄŸÄ±rdÄ±ktan sonra:
- Task isimlerini, priority'lerini ve deadline'larÄ±nÄ± AÃ‡IKÃ‡A yaz
- Sadece "1 task var" deme, task'Ä±n ADI ne?
- Plan yaparken somut adÄ±mlar ve tahmini sÃ¼reler ver

### 4. AKSÄ°YON ODAKLI OL
- BÃ¼yÃ¼k gÃ¶revleri kÃ¼Ã§Ã¼k adÄ±mlara bÃ¶l
- "Sadece 2 dakika" kuralÄ±nÄ± hatÄ±rlat
- BaÅŸarÄ±larÄ± kutla, baÅŸarÄ±sÄ±zlÄ±klarÄ± normalize et

## BUGÃœNÃœN TARÄ°HÄ°: ${new Date().toISOString().split('T')[0]}
- "YarÄ±n" = ${new Date(Date.now() + 86400000).toISOString().split('T')[0]}
- Eksik bilgi varsa makul varsayÄ±lanlar kullan (priority: P2)

## Ã–RNEK DAVRANIÅLAR:

### "GÃ¼nÃ¼mÃ¼ planla" dendiÄŸinde:
1. Ã–NCE get_pending_tasks veya plan_day tool'unu Ã§aÄŸÄ±r
2. SonuÃ§lardaki task isimlerini ve detaylarÄ±nÄ± oku
3. KullanÄ±cÄ±ya ÅŸÃ¶yle cevap ver:
   "Åu an 3 bekleyen task'Ä±n var:
   - [P1] Proje sunumu hazÄ±rla (yarÄ±n deadline)
   - [P2] Email'leri yanÄ±tla
   - [P3] AraÅŸtÄ±rma yap
   
   BugÃ¼n kaÃ§ saatin var? En Ã¶nemli task'la baÅŸlayalÄ±m mÄ±?"

### "Task ekle" dendiÄŸinde (belirsiz):
"Tamam, task ekleyeceÄŸim. BirkaÃ§ soru:
- Task'Ä±n adÄ± ne?
- Deadline var mÄ±?
- Ã–ncelik: P1 (acil), P2 (Ã¶nemli), P3 (normal), P4 (dÃ¼ÅŸÃ¼k)?"

### Task oluÅŸturduktan sonra:
"âœ… Task eklendi: '[Task adÄ±]' - Priority: P2, Deadline: yarÄ±n
BaÅŸka eklemek istediÄŸin var mÄ±?"

## KurallarÄ±n:
- Max 3-4 cÃ¼mle veya bullet point
- Somut, uygulanabilir Ã¶neriler ver
- "YapmalÄ±sÄ±n" yerine "Deneyebilirsin" de`

function buildContextMessage(context: ChatContext): string {
  const parts: string[] = []

  // Summary stats
  if (context.pendingTasks > 0) {
    parts.push(`ğŸ“‹ ${context.pendingTasks} bekleyen task`)
  }
  if (context.completedTasksToday > 0) {
    parts.push(`âœ… BugÃ¼n ${context.completedTasksToday} task tamamlandÄ±`)
  }
  if (context.totalHabitsToday > 0) {
    parts.push(`ğŸ¯ Habits: ${context.habitsCompletedToday}/${context.totalHabitsToday}`)
  }
  if (context.currentStreak > 0) {
    parts.push(`ğŸ”¥ ${context.currentStreak} gÃ¼nlÃ¼k streak`)
  }
  if (context.lastMood) {
    const moodEmoji = ['ğŸ˜«', 'ğŸ˜”', 'ğŸ˜', 'ğŸ™‚', 'ğŸ˜Š'][context.lastMood - 1]
    parts.push(`Mood: ${moodEmoji}`)
  }
  if (context.lastEnergy) {
    const energyEmoji = ['ğŸª«', 'ğŸ”‹', 'âš¡', 'ğŸ’ª', 'ğŸš€'][context.lastEnergy - 1]
    parts.push(`Energy: ${energyEmoji}`)
  }

  let result = ''
  if (parts.length > 0) {
    result += `\n\n[KullanÄ±cÄ± durumu: ${parts.join(' | ')}]`
  }

  // Add task list details
  if (context.taskList && context.taskList.length > 0) {
    result += '\n\n[MEVCUT TASK LÄ°STESÄ° - Bu bilgiyi kullanÄ±cÄ±ya gÃ¶ster!]'
    context.taskList.forEach((t, i) => {
      result += `\n${i + 1}. [${t.priority}] ${t.title}${t.deadline ? ` (deadline: ${t.deadline})` : ''}`
    })
  }

  // Add habit list details
  if (context.habitList && context.habitList.length > 0) {
    const pendingHabits = context.habitList.filter(h => !h.completed)
    if (pendingHabits.length > 0) {
      result += `\n\n[BUGÃœN YAPILMASI GEREKEN HABÄ°T'LER: ${pendingHabits.map(h => h.name).join(', ')}]`
    }
  }

  return result
}

interface LLMResponse {
  content: string
  toolCalls?: ToolCall[]
  error?: string
}

// OpenAI with native function calling
async function callOpenAIWithTools(
  messages: { role: string; content: string }[],
  apiKey: string,
  model: string
): Promise<LLMResponse> {
  const tools = formatToolsForOpenAI()

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model,
      messages,
      tools,
      tool_choice: 'auto',
      max_tokens: 1000,
      temperature: 0.7,
    }),
  })

  if (!response.ok) {
    const error = await response.json()
    throw new Error(error.error?.message || 'OpenAI API error')
  }

  const data = await response.json()
  const message = data.choices[0].message

  // Check for tool calls
  if (message.tool_calls && message.tool_calls.length > 0) {
    const toolCalls: ToolCall[] = message.tool_calls.map((tc: { function: { name: string; arguments: string } }) => ({
      name: tc.function.name,
      arguments: JSON.parse(tc.function.arguments),
    }))
    return {
      content: message.content || '',
      toolCalls
    }
  }

  return { content: message.content || '' }
}

// Anthropic with native tool use
async function callAnthropicWithTools(
  messages: { role: string; content: string }[],
  apiKey: string,
  model: string
): Promise<LLMResponse> {
  const systemMessage = messages.find(m => m.role === 'system')?.content || ''
  const chatMessages = messages.filter(m => m.role !== 'system')
  const tools = formatToolsForAnthropic()

  const response = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': apiKey,
      'anthropic-version': '2023-06-01',
      'anthropic-dangerous-direct-browser-access': 'true',
    },
    body: JSON.stringify({
      model,
      max_tokens: 1000,
      system: systemMessage,
      tools,
      messages: chatMessages.map(m => ({
        role: m.role as 'user' | 'assistant',
        content: m.content,
      })),
    }),
  })

  if (!response.ok) {
    const error = await response.json()
    throw new Error(error.error?.message || 'Anthropic API error')
  }

  const data = await response.json()

  // Parse response - Anthropic returns content blocks
  let textContent = ''
  const toolCalls: ToolCall[] = []

  for (const block of data.content) {
    if (block.type === 'text') {
      textContent += block.text
    } else if (block.type === 'tool_use') {
      toolCalls.push({
        name: block.name,
        arguments: block.input,
      })
    }
  }

  return {
    content: textContent,
    toolCalls: toolCalls.length > 0 ? toolCalls : undefined
  }
}

// Fallback for providers without native tool support
async function callOpenAI(
  messages: { role: string; content: string }[],
  apiKey: string,
  model: string
): Promise<LLMResponse> {
  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model,
      messages,
      max_tokens: 500,
      temperature: 0.7,
    }),
  })

  if (!response.ok) {
    const error = await response.json()
    throw new Error(error.error?.message || 'OpenAI API error')
  }

  const data = await response.json()
  return { content: data.choices[0].message.content }
}

async function callAnthropic(
  messages: { role: string; content: string }[],
  apiKey: string,
  model: string
): Promise<LLMResponse> {
  const systemMessage = messages.find(m => m.role === 'system')?.content || ''
  const chatMessages = messages.filter(m => m.role !== 'system')

  const response = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': apiKey,
      'anthropic-version': '2023-06-01',
      'anthropic-dangerous-direct-browser-access': 'true',
    },
    body: JSON.stringify({
      model,
      max_tokens: 500,
      system: systemMessage,
      messages: chatMessages.map(m => ({
        role: m.role as 'user' | 'assistant',
        content: m.content,
      })),
    }),
  })

  if (!response.ok) {
    const error = await response.json()
    throw new Error(error.error?.message || 'Anthropic API error')
  }

  const data = await response.json()
  return { content: data.content[0].text }
}

async function callGoogle(
  messages: { role: string; content: string }[],
  apiKey: string,
  model: string
): Promise<LLMResponse> {
  const systemMessage = messages.find(m => m.role === 'system')?.content || ''
  const chatMessages = messages.filter(m => m.role !== 'system')

  const contents = chatMessages.map(m => ({
    role: m.role === 'assistant' ? 'model' : 'user',
    parts: [{ text: m.content }],
  }))

  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        contents,
        systemInstruction: { parts: [{ text: systemMessage }] },
        generationConfig: { maxOutputTokens: 500, temperature: 0.7 },
      }),
    }
  )

  if (!response.ok) {
    const error = await response.json()
    throw new Error(error.error?.message || 'Google AI API error')
  }

  const data = await response.json()
  return { content: data.candidates[0].content.parts[0].text }
}

async function callGroq(
  messages: { role: string; content: string }[],
  apiKey: string,
  model: string
): Promise<LLMResponse> {
  const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model,
      messages,
      max_tokens: 500,
      temperature: 0.7,
    }),
  })

  if (!response.ok) {
    const error = await response.json()
    throw new Error(error.error?.message || 'Groq API error')
  }

  const data = await response.json()
  return { content: data.choices[0].message.content }
}

async function callOllama(
  messages: { role: string; content: string }[],
  baseUrl: string,
  model: string
): Promise<LLMResponse> {
  const response = await fetch(`${baseUrl}/api/chat`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model,
      messages,
      stream: false,
    }),
  })

  if (!response.ok) {
    throw new Error('Ollama API error - is Ollama running?')
  }

  const data = await response.json()
  return { content: data.message.content }
}

// Main send message function with native tool calling support
export async function sendMessageWithTools(
  userMessage: string,
  chatHistory: ChatMessage[],
  context: ChatContext
): Promise<AgentResponse> {
  const settings = useSettingsStore.getState()
  const { llmProvider, llmModel, apiKeys, ollamaBaseUrl } = settings

  // Check API key
  if (PROVIDER_INFO[llmProvider].requiresKey && !apiKeys[llmProvider]) {
    throw new Error(`${PROVIDER_INFO[llmProvider].name} API key gerekli. Settings'den ekleyebilirsin.`)
  }

  // Build messages array
  const contextSuffix = buildContextMessage(context)
  const messages: { role: string; content: string }[] = [
    { role: 'system', content: ADHD_COACH_SYSTEM_PROMPT + contextSuffix },
    ...chatHistory.slice(-10).map(m => ({
      role: m.role,
      content: m.content,
    })),
    { role: 'user', content: userMessage },
  ]

  // Use native tool calling for OpenAI and Anthropic
  let result: LLMResponse

  if (llmProvider === 'openai') {
    result = await callOpenAIWithTools(messages, apiKeys.openai, llmModel)
  } else if (llmProvider === 'anthropic') {
    result = await callAnthropicWithTools(messages, apiKeys.anthropic, llmModel)
  } else {
    // Fallback for other providers (no native tool support)
    const providerCalls: Record<string, () => Promise<LLMResponse>> = {
      google: () => callGoogle(messages, apiKeys.google, llmModel),
      groq: () => callGroq(messages, apiKeys.groq, llmModel),
      ollama: () => callOllama(messages, ollamaBaseUrl, llmModel),
    }
    result = await providerCalls[llmProvider]()
  }

  // Execute tool calls if present
  const toolResults: ToolResult[] = []
  if (result.toolCalls && result.toolCalls.length > 0) {
    for (const toolCall of result.toolCalls) {
      const toolResult = await executeToolCall(toolCall)
      toolResults.push(toolResult)
    }

    // If we executed tools but have no content, get a follow-up response
    if (!result.content && toolResults.length > 0) {
      // Build rich tool results summary including DATA
      const toolResultsSummary = toolResults
        .map(r => {
          let summary = `${r.success ? 'BaÅŸarÄ±lÄ±' : 'Hata'}: ${r.message}`
          // Include data details for LLM to use
          if (r.data && typeof r.data === 'object') {
            const data = r.data as Record<string, unknown>
            // For plan_day, include task details
            if (data.suggestedTasks && Array.isArray(data.suggestedTasks)) {
              const tasks = data.suggestedTasks as Array<{title: string; priority: string; deadline?: string}>
              if (tasks.length > 0) {
                summary += '\n\nÃ–ncelikli Task\'lar:'
                tasks.forEach((t, i) => {
                  summary += `\n${i + 1}. [${t.priority}] ${t.title}${t.deadline ? ` (deadline: ${t.deadline})` : ''}`
                })
              }
            }
            // For get_pending_tasks, include task list
            if (Array.isArray(r.data)) {
              const tasks = r.data as Array<{title: string; priority: string; deadline?: string | null}>
              if (tasks.length > 0) {
                summary += '\n\nBekleyen Task\'lar:'
                tasks.forEach((t, i) => {
                  summary += `\n${i + 1}. [${t.priority}] ${t.title}${t.deadline ? ` (deadline: ${t.deadline})` : ''}`
                })
              }
            }
            // For habits
            if (data.habits && typeof data.habits === 'object') {
              const habits = data.habits as {total: number; completed: number; pending: string[]}
              if (habits.pending && habits.pending.length > 0) {
                summary += `\n\nBekleyen Habit'ler: ${habits.pending.join(', ')}`
              }
            }
          }
          return summary
        })
        .join('\n\n')

      // Add tool results to messages and get a natural response
      const followUpMessages = [
        ...messages,
        { role: 'assistant', content: `[Tool Ã§aÄŸrÄ±larÄ± yapÄ±ldÄ±]\n\n${toolResultsSummary}` },
        { role: 'user', content: 'YukarÄ±daki tool sonuÃ§larÄ±nÄ± kullanarak bana DETAYLI bir cevap ver. Task isimlerini, priority\'lerini gÃ¶ster. EÄŸer planlama yapÄ±yorsan, follow-up soru sor (kaÃ§ saatin var? hangi alana odaklanmak istiyorsun?).' },
      ]

      try {
        let followUpResult: LLMResponse
        if (llmProvider === 'openai') {
          followUpResult = await callOpenAI(followUpMessages, apiKeys.openai, llmModel)
        } else if (llmProvider === 'anthropic') {
          followUpResult = await callAnthropic(followUpMessages, apiKeys.anthropic, llmModel)
        } else {
          const providerCalls: Record<string, () => Promise<LLMResponse>> = {
            google: () => callGoogle(followUpMessages, apiKeys.google, llmModel),
            groq: () => callGroq(followUpMessages, apiKeys.groq, llmModel),
            ollama: () => callOllama(followUpMessages, ollamaBaseUrl, llmModel),
          }
          followUpResult = await providerCalls[llmProvider]()
        }
        result.content = followUpResult.content
      } catch {
        // If follow-up fails, generate a simple response based on tool results
        result.content = generateSimpleResponse(toolResults)
      }
    }
  }

  return {
    content: result.content,
    toolCalls: result.toolCalls || [],
    toolResults,
  }
}

// Generate a simple response when LLM follow-up fails
function generateSimpleResponse(toolResults: ToolResult[]): string {
  const successCount = toolResults.filter(r => r.success).length
  const totalCount = toolResults.length

  if (totalCount === 0) return ''

  if (successCount === totalCount) {
    if (totalCount === 1) {
      const result = toolResults[0]
      // Generate contextual responses based on tool type
      if (result.message.includes('plan created') || result.message.includes('Day plan')) {
        return `GÃ¼nÃ¼n iÃ§in bir plan hazÄ±rladÄ±m! ğŸ“‹ ${result.message.split(':').slice(1).join(':').trim() || ''}`
      }
      if (result.message.includes('Task') && result.message.includes('created')) {
        return `Tamam, task'Ä± ekledim! âœ… BaÅŸka bir ÅŸey var mÄ±?`
      }
      if (result.message.includes('Found') && result.message.includes('results')) {
        return `Arama sonuÃ§larÄ±nÄ± buldum! ğŸ” Ä°stersen bunlarÄ± bookmark'lara kaydedebilirim.`
      }
      if (result.message.includes('bookmark') || result.message.includes('Bookmark')) {
        return `Bookmark'larÄ± kaydettim! ğŸ“š Bookmarks modÃ¼lÃ¼nden gÃ¶rebilirsin.`
      }
      return `Tamam, hallettim! âœ…`
    }
    return `${totalCount} iÅŸlem baÅŸarÄ±yla tamamlandÄ±! âœ…`
  }

  return `${successCount}/${totalCount} iÅŸlem tamamlandÄ±.`
}

// Legacy function for backward compatibility
export async function sendMessage(
  userMessage: string,
  chatHistory: ChatMessage[],
  context: ChatContext
): Promise<string> {
  const response = await sendMessageWithTools(userMessage, chatHistory, context)

  // If there were tool calls, append results to content
  if (response.toolResults.length > 0) {
    const resultSummary = response.toolResults
      .map(r => `${r.success ? 'âœ“' : 'âœ—'} ${r.message}`)
      .join('\n')

    return response.content
      ? `${response.content}\n\n---\n${resultSummary}`
      : resultSummary
  }

  return response.content
}

export function getQuickActions(): { id: string; label: string; prompt: string }[] {
  return [
    { id: 'plan', label: 'ğŸ“‹ GÃ¼nÃ¼mÃ¼ planla', prompt: 'BugÃ¼n iÃ§in bir plan yapmama yardÄ±m et. Ã–ncelikli task\'larÄ±mÄ± ve habit\'lerimi dÃ¼ÅŸÃ¼nerek basit bir gÃ¼nlÃ¼k plan Ã¶ner.' },
    { id: 'motivate', label: 'ğŸ’ª Motivasyon', prompt: 'Biraz motivasyona ihtiyacÄ±m var. KÄ±sa ve etkili bir ÅŸey sÃ¶yle.' },
    { id: 'stuck', label: 'ğŸ¤” SÄ±kÄ±ÅŸtÄ±m', prompt: 'Bir task\'a baÅŸlayamÄ±yorum, sÄ±kÄ±ÅŸtÄ±m. Ne yapabilirim?' },
    { id: 'celebrate', label: 'ğŸ‰ Kutla', prompt: 'BugÃ¼n iyi iÅŸ Ã§Ä±kardÄ±m! Benimle kutlar mÄ±sÄ±n?' },
    { id: 'break', label: 'â˜• Mola', prompt: 'Mola vermeli miyim? Ne kadar sÃ¼re Ã¶nerirsin?' },
  ]
}
